# Chapter 11: Residual networks

### 11.1.1 Limitations of sequential processing

Shattering gradients problem

### 11.4.1 Costs and benefits of batch norm

- Stable forward propagation
- Higher learning rates
- Regularization

### 11.5.2. DenseNet

Instead of adding inputs to their transformed versions, we can concatenate them. This introduces more complexity.
